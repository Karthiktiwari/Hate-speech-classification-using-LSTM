{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86418970-3792-4015-9f44-d5ac8173bd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x19a631eddf0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import random\n",
    "import time\n",
    "from torchtext.legacy.data import LabelField, Field, TabularDataset, BucketIterator\n",
    "SEED=42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecadd904-af0e-4e7a-b891-d9a4f154ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"C:\\Users\\win10\\Downloads\\Datasets\\HS Dataset\\labeled_data.csv\"\n",
    "# df = pd.read_csv(path)\n",
    "\n",
    "# subset = {'tweet':[],'label':[]}\n",
    "# hcount,ocount, ncount = 0,0,0\n",
    "# for tweet, label in zip(df['tweet'],df['class']):\n",
    "#     if(label == 0 and hcount<=1400):\n",
    "#         hcount += 1\n",
    "#         subset['tweet'].append(tweet)\n",
    "#         subset['label'].append(label)\n",
    "#     elif(label == 1 and ocount<=1400):\n",
    "#         ocount += 1\n",
    "#         subset['tweet'].append(tweet)\n",
    "#         subset['label'].append(label)\n",
    "#     elif(label == 2 and ncount<=1400):\n",
    "#         ncount+= 1\n",
    "#         subset['tweet'].append(tweet)\n",
    "#         subset['label'].append(label)\n",
    "\n",
    "# dataset = pd.DataFrame(data = subset, columns = ['tweet','label'])\n",
    "# dataset.head(10)\n",
    "\n",
    "# dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# dataset.head(10)\n",
    "\n",
    "# dataset.to_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exterior-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = Field(sequential=True, use_vocab=True, tokenize = 'spacy',tokenizer_language = 'en_core_web_sm', include_lengths=True, lower=True)\n",
    "class_label = LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cloudy-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {'tweet': ('text',tweet), 'label':('c', class_label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "possible-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"dataset.csv\"\n",
    "data= TabularDataset(path=path, format = 'CSV', fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interim-completion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['\"',\n",
       "  '@dgotbricks',\n",
       "  ':',\n",
       "  'what',\n",
       "  'happen',\n",
       "  'to',\n",
       "  'them',\n",
       "  'vixen',\n",
       "  'ent',\n",
       "  'bitches',\n",
       "  '\"',\n",
       "  'they',\n",
       "  'got',\n",
       "  'ran',\n",
       "  'and',\n",
       "  'threw',\n",
       "  'to',\n",
       "  'the',\n",
       "  'side',\n",
       "  'like',\n",
       "  'a',\n",
       "  'foothill',\n",
       "  'bitch'],\n",
       " '1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randint(0,4200)\n",
    "data.examples[i].text, data.examples[i].c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "judicial-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = random.getstate()\n",
    "train_data, test_data = data.split(split_ratio=0.85, random_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chinese-czech",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@devilgrimz',\n",
       " 'ironic',\n",
       " 'that',\n",
       " 'he',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'trashcan',\n",
       " 'because',\n",
       " 'he',\n",
       " 'holds',\n",
       " 'trash',\n",
       " 'everytime',\n",
       " 'you',\n",
       " 'two',\n",
       " 'cuddle',\n",
       " '!',\n",
       " 'burn',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[654].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3863b8e8-dcd2-4ae4-8697-9ef6e10b8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.build_vocab(train_data, max_size = 10000, min_freq = 15)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72971bda-d212-40aa-94ce-617145825a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'1': 0, '0': 1, '2': 2})\n"
     ]
    }
   ],
   "source": [
    "class_label.build_vocab(train_data)\n",
    "print(class_label.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "violent-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data,test_data),\n",
    "    batch_size = 8,\n",
    "    sort = False,\n",
    "    device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "herbal-sauce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for batch in train_iterator:\n",
    "#     print(batch.text)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "southwest-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.build_vocab(train_data)\n",
    "tweet.vocab.load_vectors('glove.6B.300d')\n",
    "embedding = tweet.vocab.vectors.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unexpected-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "korean-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,batch in enumerate(train_iterator):\n",
    "#     c = batch.c\n",
    "#     a,b = batch.text\n",
    "#     print(i,a,b,c)\n",
    "#     break\n",
    "# #     if(i==2):\n",
    "# #         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "broke-accused",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11981, 300])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tracked-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size, batch_size, embedding_weights, bidirectional = False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(embedding_weights)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            bidirectional = bidirectional,batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim*2, label_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, label_size)\n",
    "#         self.act = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, sentence, src_len, train = True):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embeds, src_len.cpu(), enforce_sorted=False)\n",
    "        packed_outputs, (hidden,cell) = self.lstm(packed_embedded)\n",
    "#         hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "#         print(hidden.shape)\n",
    "        dense_outputs = self.fc(hidden)\n",
    "        outputs=dense_outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "foster-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "nlabel = 3\n",
    "hidden_dim = 25\n",
    "\n",
    "model = LSTMClassifier(embedding_dim=embedding.shape[1],hidden_dim=hidden_dim,label_size=nlabel, batch_size=BATCH_SIZE, embedding_weights=embedding)\n",
    "model = model.to(device)\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    " \n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "opponent-optics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 148.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 1 = 0.7032023182584669\n",
      "accuracy on epoch 1 = 0.7095637584159304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 180.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 2 = 0.3742556309773351\n",
      "accuracy on epoch 2 = 0.866442953046803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 192.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 3 = 0.2857627539126665\n",
      "accuracy on epoch 3 = 0.8996085011185683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 197.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 4 = 0.2294928843251344\n",
      "accuracy on epoch 4 = 0.9186241610738255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 193.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 5 = 0.20256204116814489\n",
      "accuracy on epoch 5 = 0.9247762863534675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 183.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 6 = 0.1549125896241741\n",
      "accuracy on epoch 6 = 0.9468680089485458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 179.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 7 = 0.1431274355740247\n",
      "accuracy on epoch 7 = 0.9449664429663545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 175.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 8 = 0.10381093863824953\n",
      "accuracy on epoch 8 = 0.9678411633109619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 170.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 9 = 0.09442882517808986\n",
      "accuracy on epoch 9 = 0.9720357941834452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 139.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 10 = 0.07112400143139556\n",
      "accuracy on epoch 10 = 0.9791387024875189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 146.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 11 = 0.04978848827059508\n",
      "accuracy on epoch 11 = 0.9860178970917226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 156.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 12 = 0.09214899240994526\n",
      "accuracy on epoch 12 = 0.9706375838926175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 140.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 13 = 0.04538241916465556\n",
      "accuracy on epoch 13 = 0.9872483221743198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 177.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 14 = 0.05146512244171744\n",
      "accuracy on epoch 14 = 0.9885346756152126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 180.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 15 = 0.030187209481016632\n",
      "accuracy on epoch 15 = 0.9921700223713646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs=15\n",
    "for epoch in range(epochs):\n",
    "    time.sleep(1)\n",
    "    total_loss = 0.0\n",
    "    total_acc=0.0\n",
    "    for i, batch in enumerate(tqdm(train_iterator)):\n",
    "        (feature, batch_length), label = batch.text, batch.c\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(feature, batch_length).squeeze()\n",
    "        loss = loss_function(output, label)\n",
    "        acc=categorical_accuracy(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc+=acc.item() \n",
    "\n",
    "    print(f\"loss on epoch {epoch+1} = {total_loss/len(train_iterator)}\")\n",
    "    print(f\"accuracy on epoch {epoch+1} = {total_acc/len(train_iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fixed-instruction",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim = 1)\n",
    "_, idx = torch.max(m(output[0].reshape(1,3)), dim=1)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fed0df6e-ba08-4495-bc43-b52524b78048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.8745, -0.4071, -4.6319],\n",
       "        [ 6.1766, -0.7928, -5.5313],\n",
       "        [-1.7001, -2.7845,  3.3825],\n",
       "        [ 6.3392, -0.4266, -5.9746],\n",
       "        [ 2.4014,  3.6090, -7.6764],\n",
       "        [-3.7703, -2.5575,  4.2581],\n",
       "        [-2.6498,  4.3665, -2.7020],\n",
       "        [ 3.7369, -1.1688, -2.7710]], device='cuda:0',\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3df81f4a-9cb7-4c7b-9305-449b4978d41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 0, 1, 2, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "722f8c89-1e5e-4cd9-aa7c-ae05ff3127f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unknown-suspension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 2, 0, 1, 2, 1, 0], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d38cb92f-4755-4749-8275-612970d59f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0361, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(output,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3603adb4-1961-4f9e-88dd-fe7864c284e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_class(model, sentence, min_len = 4):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [tweet.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = model(tensor, torch.tensor(tensor.shape[0], dtype = torch.int64).unsqueeze(0))\n",
    "#     _, idx = torch.max(m(output[0].reshape(1,3)), dim=1)\n",
    "    max_preds = preds[0].argmax(dim = 1)\n",
    "#     print(max_preds)\n",
    "    return max_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "executed-novel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {'1': 0, '0': 1, '2': 2})"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_label.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "eb8b1ccc-c3d9-44b8-b171-18ddc2ea9f36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "534  \" i txt my old bitch my new bitch pussy wetter \"\n",
      "Predicted class is: 0 = Offensive. Target is: Offensive\n"
     ]
    }
   ],
   "source": [
    "i = random.randint(0,629)\n",
    "sentence = test_data[i].text\n",
    "\n",
    "sen = \"\"\n",
    "for word in sentence:\n",
    "    sen = sen +\" \" + word\n",
    "\n",
    "target_class = test_data[i].c\n",
    "\n",
    "pred_class = predict_class(model, sen).item()\n",
    "print(pred_class)\n",
    "itol = {'0': 'Hate Speech', '1': 'Offensive', '2': 'Neither'}\n",
    "print(str(i)+' '+sen)\n",
    "print(f'Predicted class is: {pred_class} = {itol[class_label.vocab.itos[pred_class]]}. Target is: {itol[target_class]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "30b9e3b9-a2c0-4fde-bbc4-966f2eb82d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is a fag\n",
      "Predicted class is: 1 = Hate Speech.\n"
     ]
    }
   ],
   "source": [
    "test_sen = 'Model is a fag'\n",
    "pred_class = predict_class(model, test_sen).item()\n",
    "itol = {'0': 'Hate Speech', '1': 'Offensive', '2': 'Neither'}\n",
    "print(test_sen)\n",
    "print(f'Predicted class is: {pred_class} = {itol[class_label.vocab.itos[pred_class]]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3af615ae-ba4b-460a-8d86-62177ec346fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▌                                                        | 24/79 [00:00<00:00, 237.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 0, 0, 2, 0], device='cuda:0')\n",
      "tensor([0, 2, 0, 0, 2, 2, 2, 0], device='cuda:0')\n",
      "tensor([1, 2, 2, 0, 1, 1, 2, 0], device='cuda:0')\n",
      "tensor([1, 2, 0, 0, 2, 0, 0, 1], device='cuda:0')\n",
      "tensor([2, 2, 2, 2, 0, 0, 1, 0], device='cuda:0')\n",
      "tensor([0, 0, 1, 1, 1, 2, 2, 1], device='cuda:0')\n",
      "tensor([0, 0, 1, 1, 0, 0, 0, 1], device='cuda:0')\n",
      "tensor([2, 2, 2, 2, 2, 2, 0, 0], device='cuda:0')\n",
      "tensor([2, 0, 0, 1, 1, 0, 2, 1], device='cuda:0')\n",
      "tensor([0, 1, 1, 2, 0, 2, 1, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 2, 2, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 0, 2, 2, 0, 2, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 0, 0, 0, 1, 2], device='cuda:0')\n",
      "tensor([2, 1, 0, 0, 2, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 2, 0, 2, 2, 1, 0, 2], device='cuda:0')\n",
      "tensor([1, 2, 2, 0, 2, 0, 1, 2], device='cuda:0')\n",
      "tensor([1, 1, 2, 0, 2, 0, 1, 2], device='cuda:0')\n",
      "tensor([0, 2, 0, 2, 0, 0, 0, 2], device='cuda:0')\n",
      "tensor([1, 1, 2, 1, 1, 2, 1, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 1, 0, 0, 1, 0], device='cuda:0')\n",
      "tensor([1, 0, 0, 2, 0, 2, 1, 1], device='cuda:0')\n",
      "tensor([1, 0, 2, 2, 2, 0, 1, 2], device='cuda:0')\n",
      "tensor([2, 1, 1, 0, 0, 2, 1, 1], device='cuda:0')\n",
      "tensor([2, 0, 0, 0, 1, 0, 1, 1], device='cuda:0')\n",
      "tensor([2, 0, 1, 0, 2, 2, 2, 0], device='cuda:0')\n",
      "tensor([2, 2, 1, 0, 2, 0, 1, 0], device='cuda:0')\n",
      "tensor([0, 2, 2, 2, 1, 2, 1, 0], device='cuda:0')\n",
      "tensor([2, 1, 0, 2, 2, 2, 0, 0], device='cuda:0')\n",
      "tensor([2, 0, 1, 2, 2, 2, 1, 1], device='cuda:0')\n",
      "tensor([0, 2, 0, 0, 1, 2, 0, 0], device='cuda:0')\n",
      "tensor([2, 2, 1, 1, 1, 2, 2, 2], device='cuda:0')\n",
      "tensor([2, 2, 2, 2, 2, 0, 0, 2], device='cuda:0')\n",
      "tensor([0, 1, 0, 1, 2, 2, 2, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 0, 2, 1, 1, 0], device='cuda:0')\n",
      "tensor([0, 1, 1, 1, 2, 2, 0, 1], device='cuda:0')\n",
      "tensor([1, 1, 1, 2, 1, 0, 2, 2], device='cuda:0')\n",
      "tensor([0, 0, 0, 2, 2, 1, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 2, 2, 2, 0, 1, 1], device='cuda:0')\n",
      "tensor([2, 2, 0, 1, 2, 1, 0, 2], device='cuda:0')\n",
      "tensor([1, 2, 1, 1, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 1, 0, 0, 0, 2, 0, 0], device='cuda:0')\n",
      "tensor([2, 0, 1, 0, 1, 0, 0, 0], device='cuda:0')\n",
      "tensor([1, 2, 1, 0, 2, 1, 0, 2], device='cuda:0')\n",
      "tensor([0, 2, 0, 1, 1, 2, 2, 2], device='cuda:0')\n",
      "tensor([0, 1, 1, 2, 0, 0, 2, 1], device='cuda:0')\n",
      "tensor([0, 1, 0, 2, 0, 1, 1, 0], device='cuda:0')\n",
      "tensor([1, 0, 1, 2, 1, 0, 1, 1], device='cuda:0')\n",
      "tensor([2, 2, 0, 0, 0, 2, 2, 0], device='cuda:0')\n",
      "tensor([0, 2, 2, 1, 2, 2, 2, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 1, 1, 0, 0, 2], device='cuda:0')\n",
      "tensor([2, 2, 2, 0, 2, 1, 2, 1], device='cuda:0')\n",
      "tensor([2, 2, 0, 2, 0, 2, 1, 0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 283.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 0, 0, 1, 0, 2, 0], device='cuda:0')\n",
      "tensor([2, 1, 0, 2, 2, 1, 1, 2], device='cuda:0')\n",
      "tensor([0, 0, 1, 1, 2, 0, 2, 2], device='cuda:0')\n",
      "tensor([0, 0, 2, 0, 1, 0, 0, 1], device='cuda:0')\n",
      "tensor([2, 0, 0, 1, 1, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 0, 2, 0, 2, 1, 0, 2], device='cuda:0')\n",
      "tensor([1, 1, 1, 2, 1, 1, 0, 2], device='cuda:0')\n",
      "tensor([1, 2, 1, 2, 1, 2, 1, 1], device='cuda:0')\n",
      "tensor([2, 1, 1, 2, 1, 2, 0, 1], device='cuda:0')\n",
      "tensor([1, 2, 2, 0, 2, 2, 2, 0], device='cuda:0')\n",
      "tensor([1, 2, 2, 2, 1, 0, 2, 1], device='cuda:0')\n",
      "tensor([1, 2, 1, 0, 1, 2, 0, 0], device='cuda:0')\n",
      "tensor([2, 0, 2, 1, 1, 2, 0, 2], device='cuda:0')\n",
      "tensor([0, 1, 0, 1, 2, 1, 1, 1], device='cuda:0')\n",
      "tensor([0, 2, 2, 0, 1, 0, 0, 2], device='cuda:0')\n",
      "tensor([2, 0, 1, 0, 0, 2, 0, 0], device='cuda:0')\n",
      "tensor([2, 2, 0, 1, 1, 2, 1, 0], device='cuda:0')\n",
      "tensor([0, 2, 1, 2, 0, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 2, 0, 1, 2, 2, 2, 2], device='cuda:0')\n",
      "tensor([2, 2, 1, 0, 0, 0, 2, 1], device='cuda:0')\n",
      "tensor([1, 2, 1, 2, 0, 1, 1, 1], device='cuda:0')\n",
      "tensor([2, 2, 2, 1, 1, 1, 0, 0], device='cuda:0')\n",
      "tensor([1, 1, 0, 2, 1, 2, 0, 0], device='cuda:0')\n",
      "tensor([2, 0, 0, 1, 1, 0, 1, 0], device='cuda:0')\n",
      "tensor([0, 0, 1, 1, 0, 0, 1, 1], device='cuda:0')\n",
      "tensor([0, 0, 0, 2, 1, 1, 1, 0], device='cuda:0')\n",
      "tensor([2, 1, 0, 0, 1, 1], device='cuda:0')\n",
      "accuracy = 0.8691983124877833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_acc=0.0\n",
    "m = nn.LogSoftmax(dim = 1)\n",
    "for i, batch in enumerate(tqdm(test_iterator)):\n",
    "    (feature, batch_length), label = batch.text, batch.c\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(feature, batch_length).squeeze()\n",
    "    acc=categorical_accuracy(output,label)\n",
    "    total_acc+=acc.item() \n",
    "print(f\"accuracy = {total_acc/len(test_iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1ade8fb-f288-4bd7-9ae3-2ecfc800bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get count of examples of each class from dataset\n",
    "# h, o, n = 0,0,0\n",
    "# for example in train_data:\n",
    "#     if(int(example.c)==0):\n",
    "#         h += 1\n",
    "#     elif(int(example.c)==1):\n",
    "#         o += 1\n",
    "#     elif(int(example.c)==2):\n",
    "#         n += 1\n",
    "\n",
    "# h,o,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc6d74-f81c-48f6-982c-6c212730ef77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
