{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86418970-3792-4015-9f44-d5ac8173bd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1e6ee1cde10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "import torch.functional as F\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import random\n",
    "import time\n",
    "from torchtext.legacy.data import LabelField, Field, TabularDataset, BucketIterator\n",
    "SEED=42\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecadd904-af0e-4e7a-b891-d9a4f154ae8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = r\"C:\\Users\\win10\\Downloads\\Datasets\\HS Dataset\\labeled_data.csv\"\n",
    "# df = pd.read_csv(path)\n",
    "\n",
    "# subset = {'tweet':[],'label':[]}\n",
    "# hcount,ocount, ncount = 0,0,0\n",
    "# for tweet, label in zip(df['tweet'],df['class']):\n",
    "#     if(label == 0 and hcount<=1400):\n",
    "#         hcount += 1\n",
    "#         subset['tweet'].append(tweet)\n",
    "#         subset['label'].append(label)\n",
    "#     elif(label == 1 and ocount<=1400):\n",
    "#         ocount += 1\n",
    "#         subset['tweet'].append(tweet)\n",
    "#         subset['label'].append(label)\n",
    "#     elif(label == 2 and ncount<=1400):\n",
    "#         ncount+= 1\n",
    "#         subset['tweet'].append(tweet)\n",
    "#         subset['label'].append(label)\n",
    "\n",
    "# dataset = pd.DataFrame(data = subset, columns = ['tweet','label'])\n",
    "# dataset.head(10)\n",
    "\n",
    "# dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# dataset.head(10)\n",
    "\n",
    "# dataset.to_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exterior-programming",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: x.split()\n",
    "tweet = Field(sequential=True, use_vocab=True, tokenize=tokenize, include_lengths=True, lower=True)\n",
    "class_label = LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cloudy-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = {'tweet': ('text',tweet), 'label':('c', class_label)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "possible-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"dataset.csv\"\n",
    "data= TabularDataset(path=path, format = 'CSV', fields = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "interim-completion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['@intheovenjews',\n",
       "  '@slavecatcher88',\n",
       "  '@marylene58',\n",
       "  'we',\n",
       "  'should',\n",
       "  'hang',\n",
       "  'out',\n",
       "  'ovenjew.',\n",
       "  'have',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'in',\n",
       "  'common.',\n",
       "  'people',\n",
       "  'waste',\n",
       "  'time',\n",
       "  'on',\n",
       "  'nigs.',\n",
       "  'jews',\n",
       "  'r',\n",
       "  'r',\n",
       "  'problem.'],\n",
       " '0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.randint(0,4200)\n",
    "data.examples[i].text, data.examples[i].c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "judicial-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = random.getstate()\n",
    "train_data, test_data = data.split(split_ratio=0.85, random_state=state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chinese-czech",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@danram70',\n",
       " 'pussy.',\n",
       " \"it's\",\n",
       " 'science.',\n",
       " 'donaire',\n",
       " 'via',\n",
       " 'rachel',\n",
       " 'spelling',\n",
       " 'error']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[654].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3863b8e8-dcd2-4ae4-8697-9ef6e10b8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.build_vocab(train_data, max_size = 10000, min_freq = 2)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72971bda-d212-40aa-94ce-617145825a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'1': 0, '0': 1, '2': 2})\n"
     ]
    }
   ],
   "source": [
    "class_label.build_vocab(train_data)\n",
    "print(class_label.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "violent-premiere",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_data,test_data),\n",
    "    batch_size = 8,\n",
    "    sort = False,\n",
    "    device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "herbal-sauce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for batch in train_iterator:\n",
    "#     print(batch.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "southwest-holmes",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.build_vocab(train_data)\n",
    "tweet.vocab.load_vectors('glove.6B.300d')\n",
    "embedding = tweet.vocab.vectors.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unexpected-click",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "korean-helmet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[ 3597,  6896,  6695,  4572,    13,  1468,   886,  2366],\n",
      "        [   33,     4,  7590,    35,  7659,  1480,    92,  7732],\n",
      "        [  179,   282,  7524,    33,  1714,  6034,     5,  7746],\n",
      "        [  188,   411,    27,     2,  7931,  1135,   164,   476],\n",
      "        [    2,  8043, 12500,   152,     9,   241,    17,    45],\n",
      "        [ 2418,    43,  9745,   462,    40,   357,     3,    63],\n",
      "        [   16,   561,  9760,     1,     2,    62,   809,   141],\n",
      "        [    3,     4,  9341,     1,  3153,    52, 12929, 10528],\n",
      "        [13428,  8354,     3,     1,    77,     1,   181,   103],\n",
      "        [    7,    10,   232,     1,     2,     1,  9677,  1076],\n",
      "        [   95, 12258,   484,     1,  9154,     1,  2679,    45],\n",
      "        [  162,  8065, 14288,     1,  7032,     1,     8,    41],\n",
      "        [   55,     7,  8120,     1,  6941,     1,     1,  1507],\n",
      "        [   35,   142,    60,     1,  6636,     1,     1,    19],\n",
      "        [   11,  1283,   739,     1,  6252,     1,     1,  1143],\n",
      "        [   44,    68,     1,     1,  4543,     1,     1,    32],\n",
      "        [   43,     9,     1,     1, 10870,     1,     1,    94],\n",
      "        [   16,    11,     1,     1,     1,     1,     1,    38],\n",
      "        [   73,  3019,     1,     1,     1,     1,     1,  9171],\n",
      "        [  211,   176,     1,     1,     1,     1,     1,     1],\n",
      "        [    6, 13010,     1,     1,     1,     1,     1,     1],\n",
      "        [   42,  1825,     1,     1,     1,     1,     1,     1],\n",
      "        [   64,    88,     1,     1,     1,     1,     1,     1],\n",
      "        [12856,    28,     1,     1,     1,     1,     1,     1],\n",
      "        [ 8642,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [  324,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [  318,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [ 5486,     1,     1,     1,     1,     1,     1,     1]],\n",
      "       device='cuda:0') tensor([28, 24, 15,  6, 17,  8, 12, 19], device='cuda:0') tensor([0, 2, 1, 0, 1, 1, 0, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(train_iterator):\n",
    "    c = batch.c\n",
    "    a,b = batch.text\n",
    "    print(i,a,b,c)\n",
    "    break\n",
    "#     if(i==2):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "broke-accused",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14876, 300])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tracked-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, label_size, batch_size, embedding_weights, bidirectional = False):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.word_embeddings = nn.Embedding(num_embeddings=47617, embedding_dim=300).from_pretrained(embedding_weights)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            bidirectional = bidirectional,batch_first=True)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(hidden_dim*2, label_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_dim, label_size)\n",
    "#         self.act = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, sentence, src_len, train = True):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embeds, src_len.cpu(), enforce_sorted=False)\n",
    "        packed_outputs, (hidden,cell) = self.lstm(packed_embedded)\n",
    "#         hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "#         print(hidden.shape)\n",
    "        dense_outputs = self.fc(hidden)\n",
    "        outputs=dense_outputs\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "foster-liquid",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "nlabel = 3\n",
    "hidden_dim = 30\n",
    "\n",
    "model = LSTMClassifier(embedding_dim=embedding.shape[1],hidden_dim=hidden_dim,label_size=nlabel, batch_size=BATCH_SIZE, embedding_weights=embedding)\n",
    "model = model.to(device)\n",
    " \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    " \n",
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    top_pred = preds.argmax(1, keepdim = True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "opponent-optics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 133.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 1 = 0.8747795806901834\n",
      "accuracy on epoch 1 = 0.5854026845904271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 154.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 2 = 0.6219372792665324\n",
      "accuracy on epoch 2 = 0.7505033557580355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 143.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 3 = 0.5190371991337279\n",
      "accuracy on epoch 3 = 0.7984899329125748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 138.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 4 = 0.4477175347406992\n",
      "accuracy on epoch 4 = 0.8241051454138703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 163.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 5 = 0.38039250361006\n",
      "accuracy on epoch 5 = 0.8562639821029083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 165.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 6 = 0.3218430271976413\n",
      "accuracy on epoch 6 = 0.8833892617449665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 145.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 7 = 0.27829817324560097\n",
      "accuracy on epoch 7 = 0.8984899328859061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 143.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 8 = 0.2290816826608357\n",
      "accuracy on epoch 8 = 0.9225391498881432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 160.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 9 = 0.18751316292469766\n",
      "accuracy on epoch 9 = 0.9404362416107382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 142.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 10 = 0.15171281749620133\n",
      "accuracy on epoch 10 = 0.9521812080536913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 163.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 11 = 0.15590324761425156\n",
      "accuracy on epoch 11 = 0.9483780760893086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:03<00:00, 148.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 12 = 0.13434092817782223\n",
      "accuracy on epoch 12 = 0.9556487696016128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 158.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 13 = 0.10545743963268629\n",
      "accuracy on epoch 13 = 0.9661633109619687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 156.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 14 = 0.07355293066744012\n",
      "accuracy on epoch 14 = 0.9798657718120806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 447/447 [00:02<00:00, 161.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 15 = 0.061286374211797834\n",
      "accuracy on epoch 15 = 0.9812639821029083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "epochs=10\n",
    "for epoch in range(epochs):\n",
    "    time.sleep(1)\n",
    "    total_loss = 0.0\n",
    "    total_acc=0.0\n",
    "    for i, batch in enumerate(tqdm(train_iterator)):\n",
    "        (feature, batch_length), label = batch.text, batch.c\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(feature, batch_length).squeeze()\n",
    "        loss = loss_function(output, label)\n",
    "        acc=categorical_accuracy(output,label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc+=acc.item() \n",
    "\n",
    "    print(f\"loss on epoch {epoch+1} = {total_loss/len(train_iterator)}\")\n",
    "    print(f\"accuracy on epoch {epoch+1} = {total_acc/len(train_iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fixed-instruction",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.LogSoftmax(dim = 1)\n",
    "_, idx = torch.max(m(output[0].reshape(1,3)), dim=1)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fed0df6e-ba08-4495-bc43-b52524b78048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2706,  1.3862,  0.3607],\n",
       "        [ 5.3656, -1.9460, -5.0985],\n",
       "        [-3.3283,  6.7847, -5.1253],\n",
       "        [ 1.3360,  0.1867, -2.3957],\n",
       "        [ 3.5087, -2.3970, -3.5488],\n",
       "        [ 4.4981, -2.3366, -3.8062],\n",
       "        [-4.9746, -0.4672,  4.9777],\n",
       "        [-3.4076,  4.4987, -2.3804]], device='cuda:0',\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3df81f4a-9cb7-4c7b-9305-449b4978d41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 0, 0, 2, 1], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "722f8c89-1e5e-4cd9-aa7c-ae05ff3127f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unknown-suspension",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1, 0, 0, 0, 2, 1], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d38cb92f-4755-4749-8275-612970d59f52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0787, device='cuda:0', grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function(output,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3603adb4-1961-4f9e-88dd-fe7864c284e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def predict_class(model, sentence, min_len = 4):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [tweet.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    preds = model(tensor, torch.tensor(1, dtype = torch.int64).unsqueeze(0))\n",
    "    _, idx = torch.max(m(output[0].reshape(1,3)), dim=1)\n",
    "#     max_preds = preds[0].argmax(dim = 1)\n",
    "#     print(max_preds)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "executed-novel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {'1': 0, '0': 1, '2': 2})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_label.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fef36132-b8a2-4a10-aed2-715fdc34795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class is: tensor([1], device='cuda:0') = Hate Speech\n"
     ]
    }
   ],
   "source": [
    "pred_class = predict_class(model, sen)\n",
    "itol = {'0': 'Hate Speech', '1': 'Offensive', '2': 'Neither'}\n",
    "print(f'Predicted class is: {pred_class} = {itol[class_label.vocab.itos[pred_class]]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3af615ae-ba4b-460a-8d86-62177ec346fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 339.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.7489451479308212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "total_acc=0.0\n",
    "m = nn.LogSoftmax(dim = 1)\n",
    "for i, batch in enumerate(tqdm(test_iterator)):\n",
    "    (feature, batch_length), label = batch.text, batch.c\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    output = model(feature, batch_length).squeeze()\n",
    "    acc=categorical_accuracy(output,label)\n",
    "    total_acc+=acc.item() \n",
    "print(f\"accuracy = {total_acc/len(test_iterator)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e06f1ac-dda3-4840-9ae2-f018ccaf413c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, o, n = 0,0,0\n",
    "for example in train_data:\n",
    "    if(int(example.c)==0):\n",
    "        h += 1\n",
    "    elif(int(example.c)==1):\n",
    "        o += 1\n",
    "    elif(int(example.c)==2):\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94dcc2-8a4e-4ce1-b626-5f30ae72757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "h,o,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1267436-c9a5-4708-a316-b5d0f73b9324",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, o, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b736802-684b-4233-987c-05f5e11dbf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_label.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c7189c8-0b70-4e0b-bdae-cd1f72a1cbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = test_data[100].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5db84062-3cfc-46bf-a3b5-2ffa2a484bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = \"\"\n",
    "for word in sentence:\n",
    "    sen = sen +\" \" + word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bea8b541-d239-480a-a122-360f0308c3b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \"@rishaawilliams: @treshon_scrooge @_nevertrustt ol bitch ass\" http://t.co/amcoiny0wg'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a09c6052-aa2c-4b43-be2f-c384c0932f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"@rishaawilliams:',\n",
       " '@treshon_scrooge',\n",
       " '@_nevertrustt',\n",
       " 'ol',\n",
       " 'bitch',\n",
       " 'ass\"',\n",
       " 'http://t.co/amcoiny0wg']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "important-irish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[100].c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-scholarship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
